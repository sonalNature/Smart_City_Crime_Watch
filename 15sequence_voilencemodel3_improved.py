# -*- coding: utf-8 -*-
"""15Sequence_VoilenceModel3_Improved.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15NE-w6nfZUDiIgESWt20I9zEWyhQRzvW
"""

from google.colab import drive
#drive.flush_and_unmount

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
from collections import deque
import matplotlib.pyplot as plt
from moviepy.editor import *
# %matplotlib inline
from sklearn.model_selection import train_test_split

#from google.colab import drive
#drive.mount('/content/drive')
#drive.flush_and_unmount

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model

# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))
violenceURL='/content/drive/MyDrive/AbuseDetaction_Folder/VoilanceAbuse/dataset/violence'
nonViolenceURL='/content/drive/MyDrive/AbuseDetaction_Folder/VoilanceAbuse/dataset/non-violence'
dirViolence = os.listdir(violenceURL)
dirNonViolence=os.listdir(nonViolenceURL)

print(len(dirViolence))
print(len(dirNonViolence))



"""# **Preprocessing**
---
"""



"""# **Preprocessing**
---
"""

# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 224,224

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 15

# Specify the directory containing dataset. 
DATASET_DIR = "/content/drive/MyDrive/AbuseDetaction_Folder/VoilanceAbuse/dataset"

# Specify the list containing the names of the classes used for training.
CLASSES_LIST = ["violence","non-violence"]

"""**Method To Extract Frame From each Vidioes**

---






"""

def frames_extraction(video_path):
 

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list

"""**Method to prepare dataset to train and test**

------
"""

def create_dataset():
   
    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)
      
    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths

# Create the dataset.

features, labels, video_files_paths = create_dataset()

import pickle
import os

"""**Save the dataset of extracted feature**

---
"""

pickle.dump(features,open("/content/drive/MyDrive/AbuseDetaction_Folder/SaveModel/224_15Featurs.dat","wb"), protocol=4)
pickle.dump(labels,open("/content/drive/MyDrive/AbuseDetaction_Folder/SaveModel/224_15labels.dat","wb"), protocol=4)

"""**Load the saved dataset Of extracted Feature**

---
"""

Loaded_feature=pickle.load(open("/content/drive/MyDrive/AbuseDetaction_Folder/SaveModel/224_15Featurs.dat","rb"))
Loaded_Label=pickle.load(open("/content/drive/MyDrive/AbuseDetaction_Folder/SaveModel/224_15labels.dat","rb"))
#df_dataset_feature=pd.DataFrame(dataset_feature,columns=['features','labels','video_files_paths'])
#print(df_dataset_feature)



from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model

one_hot_encoded_labels = to_categorical(Loaded_Label)
features_train, features_test, labels_train, labels_test = train_test_split(Loaded_feature, one_hot_encoded_labels,
                                                                            test_size = 0.20, shuffle = True)



"""**Creating InceptionV3 Model**

---
"""

BATCH_SIZE = 32
EPOCHS = 50

from tensorflow.keras import optimizers
from tensorflow import keras
import tensorflow as tf
from keras.applications.resnet_v2 import ResNet50V2
from keras.layers import Dense, LSTM,  Flatten, TimeDistributed, Conv2D, Dropout
from keras import Sequential
from keras.models import Model
from tensorflow.keras.optimizers import Adam

ResNet50V2 = ResNet50V2(
    include_top=False,
    weights='imagenet',
    input_shape=(224, 224, 3)
)# do not train first layers, I want to only train
# the 8 last layers 
for layer in ResNet50V2.layers[:-8]:
    layer.trainable = False# create a Sequential model
model = Sequential()# add vgg model for 15 input images (keeping the right shape
model.add( TimeDistributed(ResNet50V2, input_shape=(15, 224, 224, 3)))# now, flatten on each output to send 15



model.add(TimeDistributed(Dropout(0.2)))

model.add(Dense(64, activation="relu"))

model.add(TimeDistributed(Dropout(0.25)))

model.add(Dense(32, activation="relu",kernel_regularizer=keras.regularizers.l2(0.001)))

model.add(Dense(16, activation="relu"))
                                      
model.add(TimeDistributed(Flatten()))
                                      
model.add(LSTM(64, activation='relu', return_sequences=False))

model.add(Dropout(0.2))
model.add(Dropout(0.2))


model.add(Dense(2, activation='sigmoid'))
model.compile('adam', loss='binary_crossentropy', metrics=["accuracy"])

model.summary()

# Plot the structure of the contructed model.
plot_model(model, to_file = 'ResNet50V2_model_structure_plot.png', show_shapes = True, show_layer_names = True)



# Create an Instance of Early Stopping Callback
#early_stopping_callback = EarlyStopping( verbose=1, save_best_only=True)
import datetime


from tensorflow.keras.callbacks import ModelCheckpoint
# Start training the model.
#convlstm_model_training_history = model.fit(x = features_train, y = labels_train, epochs = 20,shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
x = datetime.datetime.now()

check_point=ModelCheckpoint(filepath=r"/content/drive/MyDrive/AbuseDetaction_Folder/SavedModelNew/ResNet50V2.hdf5", monitor="val_loss", verbose=1, save_best_only=True, mode="min")
callbacks = [check_point]
ResNet50V2LSTM_model_training_history=model.fit(features_train, labels_train, batch_size=5, callbacks=callbacks, epochs=50, validation_split = 0.2)



ResNet50V2LSTM_model_training_history





print(ResNet50V2LSTM_model_training_history.history.keys())

ResNet50V2LSTM_model_training_history







import matplotlib.pyplot as plt
def performance_Report(history):
    train_Loss=history.history['loss']
    val_Loss=  history.history['val_loss']
    epochs=range(1,51)
    plt.plot(epochs, train_Loss, color='blue', label='Training loss')
    plt.plot(epochs, val_Loss, color='green', label='validation loss')
    plt.title('Training and Validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    
    Acc_train = history.history['accuracy']
    Acc_val = history.history['val_accuracy']
    plt.plot(epochs, Acc_train, 'b', label='Training accuracy')
    plt.plot(epochs, Acc_val, 'g', label='validation accuracy')
    plt.title('Training and Validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('accuracy')
    plt.legend()
    plt.show()

performance_Report(ResNet50V2LSTM_model_training_history)



"""**Save Model**

"""

def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()

# Visualize the training and validation loss metrices.
plot_metric(ResNet50V2LSTM_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

# Visualize the training and validation accuracy metrices.
plot_metric(ResNet50V2LSTM_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

#Changing the 2D array to 1D for compatibility.
import numpy as np
rounded_ytest=np.argmax(labels_test, axis=1)
print(rounded_ytest.shape)

Loaddedmodel = keras.models.load_model('/content/drive/MyDrive/AbuseDetaction_Folder/SavedModelNew/ResNet50_15SeqmodelMar17.hdf5')

# generate a majority class prediction 
ns_probs = [0 for _ in range(len(rounded_ytest))]

# predict probabilities
yhat_probs = model.predict(features_test)

# keeping the probabilities for the positive outcome only
yhat_probs = yhat_probs[:, 1]

accuracy_score( ns_probs, yhat_probs)

# calculating the scores
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

ns_AUC = roc_auc_score(rounded_ytest, ns_probs)
yhat_probs_AUC = roc_auc_score(rounded_ytest, yhat_probs)

# summarize scores
print('Majority class ROC_AUC : %.3f' % (ns_AUC))
print('ResNet50 ROC_AUC : %.3f' % (yhat_probs_AUC))

# calculate the roc curve values
ns_fpr, ns_tpr, _ = roc_curve(rounded_ytest, ns_probs)
yhat_fpr, yhat_tpr, _ = roc_curve(rounded_ytest, yhat_probs)

import matplotlib.pyplot as plt

plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Normal')
plt.plot(yhat_fpr, yhat_tpr, marker='.', label='InceptionV3')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#Prediction of lables for the unseen dataset
y_pred=model.predict(features_test)
y_pred[:5]

from sklearn.metrics import confusion_matrix
import seaborn as sns

lables = ['Normal','Abuse/Violence']  
ax= plt.subplot()
cm = confusion_matrix(np.asarray(labels_test).argmax(axis=1), np.asarray(y_pred).argmax(axis=1))
sns.heatmap(cm, annot=True, fmt='g', ax=ax); 
ax.set_xlabel('Predicted labels');ax.set_ylabel('Actual labels');
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(lables); ax.yaxis.set_ticklabels(lables)

from sklearn.metrics import classification_report
y_pred = (y_pred > 0.5)
print(classification_report(labels_test, y_pred))

#Precision Score

from sklearn.metrics import precision_score

precision = precision_score((labels_test).argmax(axis=1), np.asarray(y_pred).argmax(axis=1), labels=[0,1], average='micro')
print(f'The precision score of the ResNet50 model is {round((precision*100),2)}%')

from sklearn.metrics import recall_score
recall = recall_score((labels_test).argmax(axis=1), np.asarray(y_pred).argmax(axis=1), labels=[0,1], average='micro')
print(f'The recall score of the ResNet50 model is {round((recall*100),2)}%')



from sklearn.metrics import f1_score
f1_score = f1_score((labels_test).argmax(axis=1), np.asarray(y_pred).argmax(axis=1), labels=[0,1], average='micro')
print(f'The f1_score of ResNet50 model is {round((f1_score*100),2)}%')

# Commented out IPython magic to ensure Python compatibility.
# # Discard the output of this cell.
# %%capture
# 
# # Install the required libraries.
# !pip install pafy youtube-dl moviepy
# # Import the required libraries.
# import os
# import cv2
# import pafy
# import math
# import random
# import numpy as np
# import datetime as dt
# import tensorflow as tf
# from collections import deque
# import matplotlib.pyplot as plt
# 
# from moviepy.editor import *
# %matplotlib inline





def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]
        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'/content/drive/MyDrive/AbuseDetaction_Folder/PredictedData/Project/123.mp4'
input_video_file_path = f'/content/drive/MyDrive/AbuseDetaction_Folder/PredictedData/TestData.mp4'

# Perform Action Recognition on the Test Video.
#predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
#VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()

"""FOr Multiple run
https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/
"""